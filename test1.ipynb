{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "tr = sc.textFile(\"datasets/small.txt\").zipWithUniqueId()\n",
    "k = sc.broadcast(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = tr.cartesian(tr)\n",
    "cart.saveAsTextFile('cart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances (dt):\n",
    "    idts = dt[0][1]\n",
    "    test = np.array(dt[0][0].split(\",\"))\n",
    "    ts = test[0:11].astype(np.float)\n",
    "    clts = int(test[11])\n",
    "    idtr = dt[1][1]\n",
    "    train = np.array(dt[1][0].split(\",\"))\n",
    "    tr = train[0:11].astype(np.float)\n",
    "    cltr = int(train[11])\n",
    "    \n",
    "    dist = np.sum((ts-tr)**2)\n",
    "    \n",
    "    if idts == idtr:\n",
    "        dist = float(\"inf\")\n",
    "    \n",
    "    return (idts,(dist,cltr,clts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances (dt):\n",
    "    idts = dt[0][1]\n",
    "    test = dt[0][0].split(\",\")\n",
    "    ts = list(map(float, test[0:7]))\n",
    "    clts = int(test[7])\n",
    "    idtr = dt[1][1]\n",
    "    train = dt[1][0].split(\",\")\n",
    "    tr = list(map(float, train[0:7]))\n",
    "    cltr = int(train[7])\n",
    "    \n",
    "    dist = sum((p-q)**2 for p, q in zip(ts, tr)) ** .5\n",
    "    \n",
    "    if idts == idtr:\n",
    "        dist = float(\"inf\")\n",
    "    \n",
    "    return (idts,(dist,cltr,clts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o533.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 25.0 failed 1 times, most recent failure: Lost task 3.0 in stage 25.0 (TID 63, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 1558, in func\n    for x in iterator:\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-48-d00f800ea281>\", line 5, in distances\nIndexError: index 11 is out of bounds for axis 0 with size 8\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)\n\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n\t... 41 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 1558, in func\n    for x in iterator:\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-48-d00f800ea281>\", line 5, in distances\nIndexError: index 11 is out of bounds for axis 0 with size 8\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)\n\t... 10 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-784bb26271b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The time to run is:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o533.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 25.0 failed 1 times, most recent failure: Lost task 3.0 in stage 25.0 (TID 63, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 1558, in func\n    for x in iterator:\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-48-d00f800ea281>\", line 5, in distances\nIndexError: index 11 is out of bounds for axis 0 with size 8\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)\n\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n\t... 41 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\", line 1558, in func\n    for x in iterator:\n  File \"/home/javier/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-48-d00f800ea281>\", line 5, in distances\nIndexError: index 11 is out of bounds for axis 0 with size 8\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)\n\t... 10 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "dist = cart.map(distances)\n",
    "dist.saveAsTextFile('dist')\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_klist(value):\n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    x[0] = value\n",
    "    return x\n",
    "\n",
    "def merge_klist(x, value):\n",
    "    for i in range(len(x)):\n",
    "        if value[0]<x[i][0]:\n",
    "            for j in range(len(x)-1,i,-1):\n",
    "                x[j]=x[j-1]\n",
    "            x[i]=value\n",
    "            break\n",
    "    return x\n",
    "\n",
    "def merge_combiners(x,y):\n",
    "    l = 0\n",
    "    for i in range(len(x)):\n",
    "        if y[l][0]<x[i][0]:\n",
    "            for j in range(len(x)-1,i,-1):\n",
    "                x[j]=x[j-1]\n",
    "            x[i]=y[l]\n",
    "            l+=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_klist(value):\n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    x[0] = value\n",
    "    return x\n",
    "\n",
    "def merge_klist(x, value):\n",
    "    for i in range(len(x)):\n",
    "        if value[0]<x[i][0]:\n",
    "            x.insert(i,value)\n",
    "            x.pop()\n",
    "            break\n",
    "    return x\n",
    "\n",
    "def merge_combiners(x,y):\n",
    "    l = 0\n",
    "    for i in range(len(x)):\n",
    "        if y[l][0]<x[i][0]:\n",
    "            x.insert(i,y[l])\n",
    "            x.pop()\n",
    "            l+=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = dist.combineByKey(create_klist, merge_klist, merge_combiners)\n",
    "k_vals.saveAsTextFile('k_vals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_class(dt):\n",
    "    rclass = dt[1][0][2]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "            \n",
    "    \n",
    "    return (rclass,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_class = k_vals.map(guess_class)\n",
    "guess_class.saveAsTextFile('guess_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "correct.foreach(lambda x: accum.add(x))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8154761904761905"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time to run is: 164.25576519966125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nk_vals = dist.combineByKey(create_klist, merge_klist, merge_combiners)\\nguess_class = k_vals.map(guess_class)\\n\\n\\n\\ndef correct(dt):\\n    if dt[0]==dt[1]:\\n        return 1\\n    else:\\n        return 0\\n    \\ncorrect = guess_class.map(correct)\\naccuracy = correct.mean()\\nend = time.time()\\nprint('The time to run is:', end - start)\\nprint(accuracy)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distances (dt):\n",
    "    idts = dt[0][1]\n",
    "    test = dt[0][0].split(\",\")\n",
    "    ts = list(map(float, test[0:11]))\n",
    "    clts = int(test[11])\n",
    "    idtr = dt[1][1]\n",
    "    train = dt[1][0].split(\",\")\n",
    "    tr = list(map(float, train[0:11]))\n",
    "    cltr = int(train[11])\n",
    "    \n",
    "    dist = sum((p-q)**2 for p, q in zip(ts, tr)) ** .5\n",
    "    \n",
    "    if idts == idtr:\n",
    "        dist = float(\"inf\")\n",
    "    \n",
    "    return (idts,(dist,cltr,clts))\n",
    "    \n",
    "def create_klist(value):\n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    x[0] = value\n",
    "    return x\n",
    "\n",
    "def merge_klist(x, value):\n",
    "    for i in range(len(x)):\n",
    "        if value[0]<x[i][0]:\n",
    "            x.insert(i,value)\n",
    "            x.pop()\n",
    "            break\n",
    "    return x\n",
    "\n",
    "def merge_combiners(x,y):\n",
    "    l = 0\n",
    "    for i in range(len(x)):\n",
    "        if y[l][0]<x[i][0]:\n",
    "            x.insert(i,y[l])\n",
    "            x.pop()\n",
    "            l+=1\n",
    "    return x\n",
    "\n",
    "def guess_class(dt):\n",
    "    rclass = dt[1][0][2]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "            \n",
    "    \n",
    "    return (rclass,predict)\n",
    "\n",
    "start = time.time()\n",
    "sc = SparkContext.getOrCreate()\n",
    "tr = sc.textFile(\"datasets/medium.txt\").zipWithUniqueId()\n",
    "k = sc.broadcast(1)\n",
    "\n",
    "cart = tr.cartesian(tr)\n",
    "dist = cart.map(distances)\n",
    "dist.saveAsTextFile('dist')\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)\n",
    "'''\n",
    "\n",
    "k_vals = dist.combineByKey(create_klist, merge_klist, merge_combiners)\n",
    "guess_class = k_vals.map(guess_class)\n",
    "\n",
    "\n",
    "\n",
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)\n",
    "print(accuracy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time to run is: 0.9075469970703125\n",
      "0.7916666666666665\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "def distances (dt):\n",
    "    idts = dt[0][1]\n",
    "    test = dt[0][0].split(\",\")\n",
    "    ts = list(map(float, test[0:7]))\n",
    "    clts = int(test[7])\n",
    "    idtr = dt[1][1]\n",
    "    train = dt[1][0].split(\",\")\n",
    "    tr = list(map(float, train[0:7]))\n",
    "    cltr = int(train[7])\n",
    "    \n",
    "    dist = sum((p-q)**2 for p, q in zip(ts, tr)) ** .5\n",
    "    \n",
    "    if idts == idtr:\n",
    "        dist = float(\"inf\")\n",
    "    \n",
    "    return (idts,(dist,cltr,clts))\n",
    "    \n",
    "def create_klist(value):\n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    x[0] = value\n",
    "    return x\n",
    "\n",
    "def merge_klist(x, value):\n",
    "    for i in range(len(x)):\n",
    "        if value[0]<x[i][0]:\n",
    "            for j in range(len(x)-1,i,-1):\n",
    "                x[j]=x[j-1]\n",
    "            x[i]=value\n",
    "            break\n",
    "    return x\n",
    "\n",
    "def merge_combiners(x,y):\n",
    "    l = 0\n",
    "    for i in range(len(x)):\n",
    "        if y[l][0]<x[i][0]:\n",
    "            for j in range(len(x)-1,i,-1):\n",
    "                x[j]=x[j-1]\n",
    "            x[i]=y[l]\n",
    "            l+=1\n",
    "    return x\n",
    "\n",
    "def guess_class(dt):\n",
    "    rclass = dt[1][0][2]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "            \n",
    "    \n",
    "    return (rclass,predict)\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "tr = sc.textFile(\"datasets/small.txt\").zipWithUniqueId()\n",
    "k = sc.broadcast(1)\n",
    "\n",
    "cart = tr.cartesian(tr)\n",
    "dist = cart.map(distances)\n",
    "k_vals = dist.combineByKey(create_klist, merge_klist, merge_combiners)\n",
    "guess_class = k_vals.map(guess_class)\n",
    "\n",
    "\n",
    "\n",
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ts = sc.textFile(\"datasets/small.txt\").zipWithUniqueId()\\\n",
    "    .map(lambda line: (line[0].split(','), line[1]))\\\n",
    "    .map(lambda line: (list(map(float, line[0])),line[1]))\n",
    "\n",
    "tr = ts.collect()\n",
    "k = sc.broadcast(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances (dt):\n",
    "    idts = dt[1]\n",
    "    ts = list(map(float, dt[0][0:7]))\n",
    "    cltr = int(dt[0][7])\n",
    "    \n",
    "    \n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    \n",
    "    for i in tr:\n",
    "        if not idts == i[1]:\n",
    "            dist = sum((p-q)**2 for p, q in zip(i[0][0:7], ts)) ** .5\n",
    "            for j in range(len(x)):\n",
    "                if dist < x[j][0]:\n",
    "                    for z in range(len(x)-1,j,-1):\n",
    "                        x[z]=x[z-1]\n",
    "                    x[j]=(dist,i[0][7])\n",
    "                    break\n",
    "                \n",
    "    return (cltr,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_vals = ts.map(distances)\n",
    "k_vals.saveAsTextFile('k_vals')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_class(dt):\n",
    "    rclass = dt[0]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    return (rclass,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_class = k_vals.map(guess_class)\n",
    "guess_class.saveAsTextFile('guess_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "correct.foreach(lambda x: accum.add(x))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time to run is: 2.9602646827697754 result is 72138.75518748573\n",
      "The time to run is: 2.8524391651153564 result is 72138.75518748573\n"
     ]
    }
   ],
   "source": [
    "from operator import sub\n",
    "a = [7,8,6,2,5,4,8,9]*1000000\n",
    "b = [5,8,6,2,65,4,48,9]*1000000\n",
    "import time\n",
    "start = time.time()\n",
    "dist = sum((p-q)**2 for p, q in zip(a, b)) ** .5\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start, 'result is', dist)\n",
    "start = time.time()\n",
    "dist = np.linalg.norm(list(map(sub,a,b)))\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start, 'result is', dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time to run is: 0.46677112579345703\n",
      "0.815476190476191\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "def distances (dt):\n",
    "    idts = dt[1]\n",
    "    ts = list(map(float, dt[0][0:7]))\n",
    "    cltr = int(dt[0][7])\n",
    "    \n",
    "    \n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    \n",
    "    for i in tr:\n",
    "        if not idts == i[1]:\n",
    "            dist = sum((p-q)**2 for p, q in zip(i[0][0:7], ts)) ** .5\n",
    "            for j in range(len(x)):\n",
    "                if dist < x[j][0]:\n",
    "                    for z in range(len(x)-1,j,-1):\n",
    "                        x[z]=x[z-1]\n",
    "                    x[j]=(dist,i[0][7])\n",
    "                    break\n",
    "                \n",
    "    return (cltr,x)\n",
    "\n",
    "\n",
    "def guess_class(dt):\n",
    "    rclass = dt[0]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    return (rclass,predict)\n",
    "\n",
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ts = sc.textFile(\"datasets/small.txt\").zipWithUniqueId()\\\n",
    "    .map(lambda line: (line[0].split(','), line[1]))\\\n",
    "    .map(lambda line: (list(map(float, line[0])),line[1]))\n",
    "\n",
    "tr = ts.collect()\n",
    "k = sc.broadcast(5)\n",
    "\n",
    "\n",
    "k_vals = ts.map(distances)\n",
    "guess_class = k_vals.map(guess_class)\n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time to run is: 80.82576751708984\n",
      "accuracy\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "def distances (dt):\n",
    "    idts = dt[1]\n",
    "    ts = list(map(float, dt[0][0:11]))\n",
    "    cltr = int(dt[0][11])\n",
    "    \n",
    "    \n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    \n",
    "    for i in tr:\n",
    "        if not idts == i[1]:\n",
    "            dist = sum((p-q)**2 for p, q in zip(i[0][0:11], ts)) ** .5\n",
    "            \n",
    "            for j in range(len(x)):\n",
    "                if dist < x[j][0]:\n",
    "                    for z in range(len(x)-1,j,-1):\n",
    "                        x[z]=x[z-1]\n",
    "                    x[j]=(dist,i[0][11])\n",
    "                    break\n",
    "                \n",
    "    return (cltr,x)\n",
    "\n",
    "\n",
    "def guess_class(dt):\n",
    "    rclass = dt[0]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    return (rclass,predict)\n",
    "\n",
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ts = sc.textFile(\"datasets/medium.txt\").zipWithUniqueId()\\\n",
    "    .map(lambda line: (line[0].split(','), line[1]))\\\n",
    "    .map(lambda line: (list(map(float, line[0])),line[1]))\n",
    "\n",
    "tr = ts.collect()\n",
    "k = sc.broadcast(5)\n",
    "\n",
    "\n",
    "k_vals = ts.map(distances)\n",
    "guess_class = k_vals.map(guess_class)\n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)\n",
    "print('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ts = sc.textFile(\"datasets/small.txt\").zipWithUniqueId()\\\n",
    "    .map(lambda line: (line[0].split(','), line[1]))\\\n",
    "    .map(lambda line: (array('d', list(map(float, line[0]))),line[1]))\n",
    "\n",
    "k = sc.broadcast(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances (dt):\n",
    "    idts = dt[1]\n",
    "    ts =  dt[0][0:7]\n",
    "    cltr = int(dt[0][7])\n",
    "    \n",
    "    \n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    \n",
    "    for i in tr:\n",
    "        if not idts == i[1]:\n",
    "            dist = sum((p-q)**2 for p, q in zip(i[0][0:7], ts)) ** .5\n",
    "            for j in range(len(x)):\n",
    "                if dist < x[j][0]:\n",
    "                    for z in range(len(x)-1,j,-1):\n",
    "                        x[z]=x[z-1]\n",
    "                    x[j]=(dist,i[0][7])\n",
    "                    break\n",
    "                \n",
    "    return (cltr,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = ts.map(distances)\n",
    "k_vals.saveAsTextFile('k_vals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_class(dt):\n",
    "    rclass = dt[0]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    return (rclass,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_class = k_vals.map(guess_class)\n",
    "guess_class.saveAsTextFile('guess_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "correct.foreach(lambda x: accum.add(x))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815476190476191\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time to run is: 0.35704588890075684\n",
      "0.815476190476191\n"
     ]
    }
   ],
   "source": [
    "#Fastest way until now\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def distances (dt):\n",
    "    idts = dt[1]\n",
    "    ts = list(map(float, dt[0][0:7]))\n",
    "    cltr = int(dt[0][7])\n",
    "    \n",
    "    \n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    \n",
    "    for i in tr:\n",
    "        if not idts == i[1]:\n",
    "            dist = sum((p-q)*(p-q) for p, q in zip(i[0][0:7], ts))\n",
    "            if dist < x[len(x)-1][0]:\n",
    "                for j in range(len(x)):\n",
    "                    if dist < x[j][0]:\n",
    "                        x.insert(j,(dist,i[0][7]))\n",
    "                        x.pop()\n",
    "                        break\n",
    "                \n",
    "    return (cltr,x)\n",
    "def guess_class(dt):\n",
    "    rclass = dt[0]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    return (rclass,predict)\n",
    "\n",
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ts = sc.textFile(\"datasets/small.txt\").zipWithUniqueId()\\\n",
    "    .map(lambda line: (line[0].split(','), line[1]))\\\n",
    "    .map(lambda line: (list(map(float, line[0])),line[1]))\n",
    "\n",
    "tr = ts.collect()\n",
    "k = sc.broadcast(5)\n",
    "\n",
    "\n",
    "k_vals = ts.map(distances)\n",
    "guess_class = k_vals.map(guess_class)\n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time to run is: 0.3636176586151123\n",
      "0.815476190476191\n"
     ]
    }
   ],
   "source": [
    "#Fastest way until now\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def distances (dt):\n",
    "    idts = dt[1]\n",
    "    ts = list(map(float, dt[0][0:7]))\n",
    "    cltr = int(dt[0][7])\n",
    "    \n",
    "    \n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    \n",
    "    for i in tr:\n",
    "        if not idts == i[1]:\n",
    "            dist = sum((p-q)*(p-q) for p, q in zip(i[0][0:7], ts))\n",
    "            if dist < x[len(x)-1][0]:\n",
    "                for j in range(len(x)):\n",
    "                    if dist < x[j][0]:\n",
    "                        x.insert(j,(dist,i[0][7]))\n",
    "                        x.pop()\n",
    "                        break\n",
    "                \n",
    "    return (cltr,x)\n",
    "def guess_class(dt):\n",
    "    rclass = dt[0]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    return (rclass,predict)\n",
    "\n",
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ts = sc.textFile(\"datasets/small.txt\").zipWithUniqueId()\\\n",
    "    .map(lambda line: (line[0].split(','), line[1]))\\\n",
    "    .map(lambda line: (list(map(float, line[0])),line[1]))\n",
    "\n",
    "tr = ts.collect()\n",
    "k = sc.broadcast(5)\n",
    "\n",
    "\n",
    "k_vals = ts.map(distances)\n",
    "guess_class = k_vals.map(guess_class)\n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fastest way until now\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def distances (dt):\n",
    "    idts = dt[1]\n",
    "    ts = list(map(float, dt[0][0:11]))\n",
    "    cltr = int(dt[0][11])\n",
    "    \n",
    "    \n",
    "    x = [(float('inf'),0)]*k.value\n",
    "    \n",
    "    for i in tr:\n",
    "        if not idts == i[1]:\n",
    "            dist = sum((p-q)*(p-q) for p, q in zip(i[0][0:11], ts))\n",
    "            if dist < x[len(x)-1][0]:\n",
    "                for j in range(len(x)):\n",
    "                    if dist < x[j][0]:\n",
    "                        x.insert(j,(dist,i[0][11]))\n",
    "                        x.pop()\n",
    "                        break\n",
    "                \n",
    "    return (cltr,x)\n",
    "def guess_class(dt):\n",
    "    rclass = dt[0]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    return (rclass,predict)\n",
    "\n",
    "def correct(dt):\n",
    "    if dt[0]==dt[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ts = sc.textFile(\"datasets/medium.txt\").zipWithUniqueId()\\\n",
    "    .map(lambda line: (line[0].split(','), line[1]))\\\n",
    "    .map(lambda line: (list(map(float, line[0])),line[1]))\n",
    "\n",
    "tr = ts.collect()\n",
    "k = sc.broadcast(5)\n",
    "\n",
    "\n",
    "k_vals = ts.map(distances)\n",
    "guess_class = k_vals.map(guess_class)\n",
    "correct = guess_class.map(correct)\n",
    "accuracy = correct.mean()\n",
    "end = time.time()\n",
    "print('The time to run is:', end - start)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cca7cc742a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0matributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrclassif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "data, _ = arff.loadarff(open('datasets/small.arff'))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    atributes = data[i][0:7]\n",
    "    rclassif = data[i][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('datasets/small.txt','r')\n",
    "data = []\n",
    "k = 5\n",
    "kitems= []\n",
    "for a in f:\n",
    "    data.append(list(map(float,a.split(','))))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    ts = data [i][0:7]\n",
    "    cltr = int(data[i][7])\n",
    "    x = [(float('inf'),0)]*k\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        if not i==j:\n",
    "            dist = sum((p-q)*(p-q) for p, q in zip(data[j][0:7], ts))\n",
    "            if dist < x[len(x)-1][0]:\n",
    "                for z in range(len(x)):\n",
    "                    if dist < x[z][0]:\n",
    "                        x.insert(z,(dist,data[j][7]))\n",
    "                        x.pop()\n",
    "                        break\n",
    "        \n",
    "    kitems.append(x)\n",
    "\n",
    "for z in range(len(kitems)):\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(kitems[z])):\n",
    "        tfreq = 1\n",
    "        tpredict = kitems[z][i][1]\n",
    "        for j in range(i+1,len(kitems[z])):\n",
    "            if tpredict == kitems[z][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    kitems[z]=predict\n",
    "    \n",
    "right = 0\n",
    "\n",
    "for i,j in zip(kitems,data[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def guess_class(dt):\n",
    "    rclass = dt[0]\n",
    "    freq = 0\n",
    "    predict = 0\n",
    "    for i in range(len(dt[1])):\n",
    "        tfreq = 1\n",
    "        tpredict = dt[1][i][1]\n",
    "        for j in range(i+1,len(dt[1])):\n",
    "            if tpredict == dt[1][j][1]:\n",
    "                tfreq +=1\n",
    "        if tfreq > freq:\n",
    "            predict = tpredict\n",
    "            freq = tfreq\n",
    "    return (rclass,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-cbc551da5a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data[1:2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
